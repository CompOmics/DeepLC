{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:07:55.803894500Z",
     "start_time": "2025-03-24T10:07:51.980945700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Pycharm_envs\\DeepLC_ONNX\\Lib\\site-packages\\tf2onnx\\tf_loader.py:68: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Pycharm_envs\\DeepLC_ONNX\\Lib\\site-packages\\tf2onnx\\tf_loader.py:72: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "from deeplc import DeepLC\n",
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['D:\\\\OneDrive - UGent\\\\Python Codes\\\\3Projects\\\\DeepLC_ONNX\\\\deeplc\\\\mods/full_hc_PXD005573_pub_1fd8363d9af9dcad3be7553c39396960.keras']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlc = DeepLC()\n",
    "keras_model = dlc.model\n",
    "keras_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:07:55.809774500Z",
     "start_time": "2025-03-24T10:07:55.804894700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[(None, 60, 6), (None, 30, 6), (None, 55), (None, 60, 20)]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(keras_model[0])\n",
    "model.input_shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:07:56.057716500Z",
     "start_time": "2025-03-24T10:07:55.809774500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rewriter <function rewrite_constant_fold at 0x000002F3075BF380>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"deeplc.onnx\"\n",
    "spec = [\n",
    "    tf.TensorSpec([None, 60, 6], tf.float32, name=\"input_1\"),\n",
    "    tf.TensorSpec([None, 30, 6], tf.float32, name=\"input_2\"),\n",
    "    tf.TensorSpec([None, 55], tf.float32, name=\"input_3\"),\n",
    "    tf.TensorSpec([None, 60, 20], tf.float32, name=\"input_4\"),\n",
    "]\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13, output_path=onnx_model_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:07:57.459851300Z",
     "start_time": "2025-03-24T10:07:56.057716500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from onnx2torch import convert\n",
    "import torch\n",
    "\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "pytorch_onnx_model = convert(onnx_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:07:58.175157400Z",
     "start_time": "2025-03-24T10:07:57.459851300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Output: tensor([[26.4123]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_tensors = (\n",
    "    torch.randn(1, 60, 6),  # Input 1\n",
    "    torch.randn(1, 30, 6),  # Input 2\n",
    "    torch.randn(1, 55),     # Input 3\n",
    "    torch.randn(1, 60, 20)  # Input 4\n",
    ")\n",
    "\n",
    "# Run PyTorch model\n",
    "output = pytorch_onnx_model(*input_tensors)\n",
    "print(\"PyTorch Model Output:\", output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:07:58.516093100Z",
     "start_time": "2025-03-24T10:07:58.503115200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Output: [array([[-18.030432],\n",
      "       [ 43.959698],\n",
      "       [ 58.209183]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from deeplc import DeepLC\n",
    "from psm_utils.psm import PSM\n",
    "from psm_utils.psm_list import PSMList\n",
    "from psm_utils.io.peptide_record import peprec_to_proforma\n",
    "\n",
    "# Initialize DeepLC\n",
    "dlc = DeepLC()\n",
    "\n",
    "# Example peptide sequences with modifications\n",
    "peptides = [\"AAGPSLSHTSGGTQSK\", \"AAINQKLIETGER\", \"AANDAGYFNDEMAPIEVKTK\"]\n",
    "modifications = [\"\", \"6|Acetyl\", \"12|Oxidation|18|Acetyl\"]\n",
    "identifiers = [\"peptide1\", \"peptide2\", \"peptide3\"]\n",
    "\n",
    "# Convert peptide + modifications into ProForma notation\n",
    "list_of_psms = [\n",
    "    PSM(peptidoform=peprec_to_proforma(seq, mod), spectrum_id=ident)\n",
    "    for seq, mod, ident in zip(peptides, modifications, identifiers)\n",
    "]\n",
    "\n",
    "# Convert to a PSMList\n",
    "psm_list = PSMList(psm_list=list_of_psms)\n",
    "\n",
    "# Extract numerical features using DeepLC\n",
    "feature_dict = dlc.do_f_extraction_psm_list(psm_list)\n",
    "\n",
    "# Ensure extracted features match ONNX expected shape\n",
    "input_1 = np.stack(list(feature_dict[\"matrix\"].values())).astype(np.float32)  # [batch, 60, 6]\n",
    "input_2 = np.stack(list(feature_dict[\"matrix_sum\"].values())).astype(np.float32)  # [batch, 30, 6]\n",
    "input_3 = np.stack(list(feature_dict[\"matrix_all\"].values())).astype(np.float32)  # [batch, ?] (Fix applied below)\n",
    "input_4 = np.stack(list(feature_dict[\"pos_matrix\"].values())).astype(np.float32)  # [batch, ?] (Fix applied below)\n",
    "\n",
    "# üî• Fix `input_3`: Ensure it has exactly 55 features\n",
    "expected_input_3_dim = 55\n",
    "if input_3.shape[1] != expected_input_3_dim:\n",
    "    padded_input_3 = np.zeros((input_3.shape[0], expected_input_3_dim), dtype=np.float32)\n",
    "    padded_input_3[:, :input_3.shape[1]] = input_3  # Fill with available data\n",
    "    input_3 = padded_input_3\n",
    "\n",
    "# üî• Fix `input_4`: Ensure it is **3D with shape [batch, 60, 20]**\n",
    "expected_input_4_shape = (input_4.shape[0], 60, 20)\n",
    "\n",
    "# üõ†Ô∏è Ensure input_4 is **at least 2D**\n",
    "if input_4.ndim == 2:\n",
    "    input_4 = np.expand_dims(input_4, axis=-1)  # Convert [batch, X] ‚Üí [batch, X, 1]\n",
    "\n",
    "# üõ†Ô∏è Now, reshape or pad to exactly `[batch, 60, 20]`\n",
    "padded_input_4 = np.zeros(expected_input_4_shape, dtype=np.float32)\n",
    "\n",
    "# Find minimum matching dimensions\n",
    "min_dim1 = min(input_4.shape[1], 60)\n",
    "min_dim2 = min(input_4.shape[2], 20)\n",
    "\n",
    "# Fill with available data\n",
    "padded_input_4[:, :min_dim1, :min_dim2] = input_4[:, :min_dim1, :min_dim2]\n",
    "input_4 = padded_input_4  # Replace with corrected array\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model_path = \"deeplc.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Prepare inputs for ONNX\n",
    "onnx_inputs = {\n",
    "    \"input_1\": input_1,  # [batch, 60, 6]\n",
    "    \"input_2\": input_2,  # [batch, 30, 6]\n",
    "    \"input_3\": input_3,  # [batch, 55]  ‚úÖ Now correctly sized\n",
    "    \"input_4\": input_4   # [batch, 60, 20]  ‚úÖ Now correctly sized\n",
    "}\n",
    "\n",
    "# Run ONNX model\n",
    "onnx_outputs = ort_session.run(None, onnx_inputs)\n",
    "\n",
    "# Print output predictions\n",
    "print(\"ONNX Model Output:\", onnx_outputs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:08:00.921188100Z",
     "start_time": "2025-03-24T10:07:59.851947100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnx2torch import convert\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import DeepLC and related utilities for feature extraction\n",
    "from deeplc import DeepLC\n",
    "from psm_utils.psm import PSM\n",
    "from psm_utils.psm_list import PSMList\n",
    "from psm_utils.io.peptide_record import peprec_to_proforma\n",
    "import copy\n",
    "\n",
    "# Set device for PyTorch computations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "csv_file = \"ATLANTIS_SILICA_fixed_mods.csv\"\n",
    "data = pd.read_csv(csv_file, keep_default_na=False)\n",
    "\n",
    "# Create a list of PSM objects from the CSV data.\n",
    "psm_list = PSMList(psm_list=[\n",
    "    PSM(peptidoform=peprec_to_proforma(row[\"seq\"], row[\"modifications\"]), spectrum_id=str(idx))\n",
    "    for idx, row in data.iterrows()\n",
    "])\n",
    "dlc = DeepLC()\n",
    "feature_dict = dlc.do_f_extraction_psm_list(psm_list)\n",
    "\n",
    "input_1 = np.stack(list(feature_dict[\"matrix\"].values())).astype(np.float32)\n",
    "input_2 = np.stack(list(feature_dict[\"matrix_sum\"].values())).astype(np.float32)\n",
    "input_3 = np.stack(list(feature_dict[\"matrix_all\"].values())).astype(np.float32)\n",
    "input_4 = np.stack(list(feature_dict[\"pos_matrix\"].values())).astype(np.float32)\n",
    "\n",
    "\n",
    "expected_input_3_dim = 55\n",
    "if input_3.shape[1] != expected_input_3_dim:\n",
    "    padded_input_3 = np.zeros((input_3.shape[0], expected_input_3_dim), dtype=np.float32)\n",
    "    padded_input_3[:, :input_3.shape[1]] = input_3\n",
    "    input_3 = padded_input_3\n",
    "\n",
    "# Ensure input_4 is a 3D tensor with shape [batch, 60, 20].\n",
    "expected_input_4_shape = (input_4.shape[0], 60, 20)\n",
    "if input_4.ndim == 2:\n",
    "    input_4 = np.expand_dims(input_4, axis=-1)\n",
    "padded_input_4 = np.zeros(expected_input_4_shape, dtype=np.float32)\n",
    "min_dim1 = min(input_4.shape[1], 60)\n",
    "min_dim2 = min(input_4.shape[2], 20)\n",
    "padded_input_4[:, :min_dim1, :min_dim2] = input_4[:, :min_dim1, :min_dim2]\n",
    "input_4 = padded_input_4\n",
    "\n",
    "y = data[\"tr\"].values.astype(np.float32)\n",
    "\n",
    "(train_input_1, test_input_1,\n",
    " train_input_2, test_input_2,\n",
    " train_input_3, test_input_3,\n",
    " train_input_4, test_input_4,\n",
    " y_train, y_test) = train_test_split(\n",
    "    input_1, input_2, input_3, input_4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "class DeepLCDataset(Dataset):\n",
    "    def __init__(self, in1, in2, in3, in4, targets):\n",
    "        self.in1 = in1\n",
    "        self.in2 = in2\n",
    "        self.in3 = in3\n",
    "        self.in4 = in4\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.in1.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.in1[idx]),\n",
    "            torch.tensor(self.in2[idx]),\n",
    "            torch.tensor(self.in3[idx]),\n",
    "            torch.tensor(self.in4[idx]),\n",
    "            torch.tensor(self.targets[idx])\n",
    "        )\n",
    "\n",
    "train_dataset = DeepLCDataset(train_input_1, train_input_2, train_input_3, train_input_4, y_train)\n",
    "test_dataset = DeepLCDataset(test_input_1, test_input_2, test_input_3, test_input_4, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:18:23.111602400Z",
     "start_time": "2025-03-24T10:18:20.396643600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss Before Transfer Learning: 1961.7255\n",
      "Epoch 1/10, Loss: 97.4451\n",
      "Epoch 2/10, Loss: 6.4018\n",
      "Epoch 3/10, Loss: 4.5538\n",
      "Epoch 4/10, Loss: 3.5531\n",
      "Epoch 5/10, Loss: 2.8361\n",
      "Epoch 6/10, Loss: 2.3910\n",
      "Epoch 7/10, Loss: 2.0271\n",
      "Epoch 8/10, Loss: 1.7389\n",
      "Epoch 9/10, Loss: 1.5321\n",
      "Epoch 10/10, Loss: 1.3355\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "onnx_model_path = \"deeplc.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "pretrained_model = convert(onnx_model)  # Convert ONNX model to PyTorch\n",
    "fine_tuned_model = copy.deepcopy(pretrained_model)  # Create a copy for fine-tuning\n",
    "pretrained_model.to(device)\n",
    "fine_tuned_model.to(device)\n",
    "\n",
    "# Note the optimizer now updates fine_tuned_model's parameters.\n",
    "optimizer = optim.Adam(fine_tuned_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = 10\n",
    "\n",
    "# Evaluation before transfer learning (using pretrained_model)\n",
    "pretrained_model.eval()\n",
    "initial_test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        in1, in2, in3, in4, targets = batch\n",
    "        in1 = in1.to(device)\n",
    "        in2 = in2.to(device)\n",
    "        in3 = in3.to(device)\n",
    "        in4 = in4.to(device)\n",
    "        targets = targets.to(device).view(-1, 1)\n",
    "        outputs = pretrained_model(in1, in2, in3, in4)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        initial_test_loss += loss.item()\n",
    "initial_test_loss /= len(test_loader)\n",
    "print(f\"Test Loss Before Transfer Learning: {initial_test_loss:.4f}\")\n",
    "\n",
    "# Fine-tuning the fine_tuned_model\n",
    "fine_tuned_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        in1, in2, in3, in4, targets = batch\n",
    "        in1 = in1.to(device)\n",
    "        in2 = in2.to(device)\n",
    "        in3 = in3.to(device)\n",
    "        in4 = in4.to(device)\n",
    "        targets = targets.to(device).view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fine_tuned_model(in1, in2, in3, in4)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:21:48.661139600Z",
     "start_time": "2025-03-24T10:20:46.923984300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8959\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        in1, in2, in3, in4, targets = batch\n",
    "        in1 = in1.to(device)\n",
    "        in2 = in2.to(device)\n",
    "        in3 = in3.to(device)\n",
    "        in4 = in4.to(device)\n",
    "        targets = targets.to(device).view(-1, 1)\n",
    "        outputs = fine_tuned_model(in1, in2, in3, in4)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:21:49.575299300Z",
     "start_time": "2025-03-24T10:21:48.662139700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1961.7255\n"
     ]
    }
   ],
   "source": [
    "pretrained_model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        in1, in2, in3, in4, targets = batch\n",
    "        in1 = in1.to(device)\n",
    "        in2 = in2.to(device)\n",
    "        in3 = in3.to(device)\n",
    "        in4 = in4.to(device)\n",
    "        targets = targets.to(device).view(-1, 1)\n",
    "        outputs = pretrained_model(in1, in2, in3, in4)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:21:50.516428600Z",
     "start_time": "2025-03-24T10:21:49.575299300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
